{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from utils import get_images\n",
    "\n",
    "# MNIST path\n",
    "mnist_path = 'data/mnist_raw/'\n",
    "x_train_num, y_train_num, x_test_num, y_test_num = get_images(mnist_path)\n",
    "\n",
    "x_train = x_train_num[:50000].reshape(50000, -1).astype(np.float32)\n",
    "y_train = y_train_num[:50000].reshape(50000, 1)\n",
    "\n",
    "x_val = x_train_num[50000:].reshape(10000, -1).astype(np.float64)\n",
    "y_val = y_train_num[50000:].reshape(10000, 1)\n",
    "\n",
    "x_test = x_test_num.copy().reshape(10000, -1).astype(np.float64)\n",
    "y_test = y_test_num.copy().reshape(10000, 1)\n",
    "\n",
    "\n",
    "def normalise(x_mean, x_std, x_data):\n",
    "    return (x_data-x_mean)/x_std\n",
    "\n",
    "x_mean = x_train.mean()\n",
    "x_std = x_train.std()\n",
    "\n",
    "x_train = normalise(x_mean, x_std, x_train)\n",
    "x_val = normalise(x_mean, x_std, x_val)\n",
    "x_test = normalise(x_mean, x_std, x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minibatches(mb_size, x, y, shuffle = True):\n",
    "    '''\n",
    "    x  #muestras, 784\n",
    "    y #muestras, 1\n",
    "    '''\n",
    "    assert x.shape[0] == y.shape[0], 'Error en cantidad de muestras'\n",
    "    total_data = x.shape[0]\n",
    "    if shuffle: \n",
    "        idxs = np.arange(total_data)\n",
    "        np.random.shuffle(idxs)\n",
    "        x = x[idxs]\n",
    "        y = y[idxs]  \n",
    "    return ((x[i:i+mb_size], y[i:i+mb_size]) for i in range(0, total_data, mb_size))\n",
    "\n",
    "class np_tensor(np.ndarray): pass\n",
    "\n",
    "#clase linear\n",
    "class Linear():\n",
    "    def __init__(self,input_size,output_size):\n",
    "        '''\n",
    "        init parameters utilizando kaiming he\n",
    "        '''        \n",
    "        self.W = (np.random.randn(output_size, input_size) /np.sqrt(input_size/2)).view(np_tensor)\n",
    "        self.b = (np.zeros((output_size,1))).view(np_tensor)\n",
    "    \n",
    "    def __call__(self,X): #forward de la clase lineal\n",
    "        z = self.W @ X + self.b\n",
    "        return z\n",
    "    \n",
    "    def backward(self,X,Z):\n",
    "        X.grad = self.W.T @ Z.grad\n",
    "        self.W.grad = Z.grad @ X.T \n",
    "        self.b.grad = np.sum(Z.grad, axis=1, keepdims=True)\n",
    "\n",
    "#clase relu\n",
    "class ReLU():\n",
    "    def __call__(self,Z):\n",
    "        return np.maximum(0,Z)\n",
    "    def backward(self,Z,a):\n",
    "        Z.grad = a.grad.copy()\n",
    "        Z.grad[Z <= 0] = 0\n",
    "\n",
    "#clase sequential\n",
    "class Sequential_layers():\n",
    "    def __init__(self,layers):\n",
    "        '''\n",
    "        layers, lista que contiene objetos de tipo Linear,ReLU\n",
    "        '''\n",
    "        self.layers = layers\n",
    "        self.x = None\n",
    "        self.outputs = {}\n",
    "\n",
    "    def __call__(self,X):\n",
    "        self.x = X \n",
    "        self.outputs['L0'] = self.x\n",
    "        for i, layer in enumerate(self.layers,1):\n",
    "            self.x = layer(self.x)\n",
    "            self.outputs['L' + str(i)] = self.x \n",
    "        return self.x \n",
    "\n",
    "    def backward(self):\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            self.layers[i].backward(self.outputs['L' + str(i)],self.outputs['L' + str(i+1)])\n",
    "\n",
    "    def update(self,learning_rate=1e-3):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, ReLU): continue\n",
    "            layer.W = layer.W - learning_rate*layer.W.grad\n",
    "            layer.b = layer.b - learning_rate*layer.b.grad\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.__call__(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function\n",
    "def softmaxXEntropy(x, y):\n",
    "    batch_size = x.shape[1]\n",
    "    exp_scores = np.exp(x)\n",
    "    probs = exp_scores / exp_scores.sum(axis = 0)\n",
    "    preds = probs.copy()\n",
    "    # Costo\n",
    "    y_hat = probs[y.squeeze(), np.arange(batch_size)]\n",
    "    cost = np.sum(-np.log(y_hat)) / batch_size\n",
    "    # Calcular gradientes\n",
    "    probs[y.squeeze(), np.arange(batch_size)] -= 1 #dl/dx\n",
    "    x.grad = probs.copy()\n",
    "    \n",
    "    return preds, cost\n",
    "\n",
    "def acurracy(x,y,mb_size):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (x,y) in enumerate(create_minibatches(mb_size,x,y)):\n",
    "        pred = model(x.T.view(np_tensor))\n",
    "        correct += np.sum(np.argmax(pred,axis=0) == y.squeeze())\n",
    "        total += pred.shape[1]\n",
    "    return correct/total\n",
    "\n",
    "def train(model,epochs,mb_size=128,learning_rate=1e-3):\n",
    "    for epoch in range(epochs):\n",
    "        for i, (x,y) in enumerate(create_minibatches(mb_size,x_train,y_train)):\n",
    "            scores = model(x.T.view(np_tensor))\n",
    "            _,cost = softmaxXEntropy(scores,y)\n",
    "            model.backward()\n",
    "            model.update(learning_rate) #parecido al step de pytorch\n",
    "        print(f'costo: {cost} ,acurracy: {acurracy(x_val,y_val,mb_size)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential_layers([Linear(784,200),ReLU(),Linear(200,10)])\n",
    "mb_size = 512\n",
    "learning_rate = 1e-4\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "costo: 0.29620508837672005 ,acurracy: 0.9109\n",
      "costo: 0.2854749359085084 ,acurracy: 0.927\n",
      "costo: 0.26910484429787174 ,acurracy: 0.9374\n",
      "costo: 0.23812861902122404 ,acurracy: 0.9456\n",
      "costo: 0.17455924045087476 ,acurracy: 0.9505\n",
      "costo: 0.15852763888610846 ,acurracy: 0.9537\n",
      "costo: 0.1878869276145521 ,acurracy: 0.9558\n",
      "costo: 0.1323894972017735 ,acurracy: 0.9581\n",
      "costo: 0.1494196775341326 ,acurracy: 0.9597\n",
      "costo: 0.13179302831067716 ,acurracy: 0.9605\n",
      "costo: 0.1155244925501145 ,acurracy: 0.9622\n",
      "costo: 0.10347332158937236 ,acurracy: 0.9637\n",
      "costo: 0.11917215050447852 ,acurracy: 0.9653\n",
      "costo: 0.10407535905162035 ,acurracy: 0.9651\n",
      "costo: 0.09431414500068903 ,acurracy: 0.9666\n",
      "costo: 0.08913059829026054 ,acurracy: 0.968\n",
      "costo: 0.10941372092214004 ,acurracy: 0.9677\n",
      "costo: 0.08801542089020027 ,acurracy: 0.9689\n",
      "costo: 0.10668923334088748 ,acurracy: 0.9687\n",
      "costo: 0.1089836856674366 ,acurracy: 0.9694\n"
     ]
    }
   ],
   "source": [
    "train(model,epochs,mb_size,learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e501ee22cc57deeb2360814fd125914d9538f135e6d01a58ac42a717d3f1d7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
